{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78723af0",
   "metadata": {
    "id": "78723af0"
   },
   "source": [
    "# Hands-on Synthetic data generation - Why and how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3b3c0",
   "metadata": {
    "id": "d9c3b3c0"
   },
   "source": [
    "### Synthetic data\n",
    "\n",
    "**Synthetic data** is artificially generated information that mimics the statistical properties of real-world data but does\n",
    "not directly correspond to real events or individuals. This type of data is created through algorithms and statistical models,\n",
    "such as generative adversarial networks (GANs) or other simulation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4b737",
   "metadata": {
    "id": "3bb4b737"
   },
   "outputs": [],
   "source": [
    "!pip install ydata-profiling==4.2.*\n",
    "!pip install ydata-synthetic==1.4.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4821ec17",
   "metadata": {
    "id": "4821ec17"
   },
   "source": [
    "### The dataset\n",
    "The data used is the [Adult Census Income](https://www.kaggle.com/datasets/uciml/adult-census-income) which we will fecth by importing the pmlb library (a wrapper for the Penn Machine Learning Benchmark data repository)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a386d95",
   "metadata": {
    "id": "4a386d95"
   },
   "outputs": [],
   "source": [
    "from pmlb import fetch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7792c88e",
   "metadata": {
    "id": "7792c88e"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = fetch_data('adult')\n",
    "num_cols = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "cat_cols = ['workclass','education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "            'native-country', 'target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8cddcd",
   "metadata": {
    "id": "ab8cddcd"
   },
   "source": [
    "### Keeping all the analysis unbiased - Holdout\n",
    "\n",
    "#### Train Test Data split and the holdout\n",
    "\n",
    "Holdout refers to a portion of historical data that is held out of the datasets used for training and validating machine learning models, some might recognize the **holdout** set as **test** dataset.\n",
    "*So why is it relevant if we are already doing a train validation set creation?*\n",
    "\n",
    "- The **train** dataset always returns model optimistic results/performance, as the model have seen the all the training data throughout the training process;\n",
    "- On the other hand, the **validation** set is still somewhat optimistic although less compared to the training set. Why? Simple it is also used to select the best model. For that reason the obtained results are somewhat biased.\n",
    "- The **holdout** or **test** dataset is completely independent form the trainign and model selection, for that reason us the best set to build unbiased performance metrics that can properly represent the behaviour of the model with new data inputs (eg. a production system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf05ba1",
   "metadata": {
    "id": "9cf05ba1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['target']\n",
    "X = data.iloc[:, data.columns != 'target']\n",
    "\n",
    "X_train, X_Hold, y_train, y_hold = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "X_train['target'] = y_train\n",
    "X_Hold['target'] = y_hold\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b054ff9",
   "metadata": {
    "id": "7b054ff9"
   },
   "source": [
    "## 1. Data profiling\n",
    "\n",
    "Data profiling (from Exploratory analysis to data monitoring) can help us to understand the data bahviour and how to best select the model and process for synthetic data generation. Does my data have missing data? So I want to keep my missing data behaviours or have it imputed during the process? What are the current distributions per variable? Are there any outliers?\n",
    "\n",
    "Data profiling, in particular, is a powerfull tool that allows to examine the data and identify not only its validity for a certain use-case but also the quality.\n",
    "\n",
    "Let's start with the overall data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a5b89",
   "metadata": {
    "id": "798a5b89"
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "report = ProfileReport(X_train,\n",
    "                       title='Census dataset',\n",
    "                       minimal=True)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8cafe",
   "metadata": {
    "id": "acc8cafe"
   },
   "source": [
    "### 1.2. Data profiling for time-series\n",
    "\n",
    "Time-series datasets have different requirements and needs in what concerns data profiling and exploratory data anaylis. Don't worry if you want to measure stationarity or check wether your data have seasonality we got you covered.\n",
    "\n",
    "Check this tutorial on [Time-series EDA](https://towardsdatascience.com/how-to-do-an-eda-for-time-series-cbb92b3b1913).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a53d30",
   "metadata": {
    "id": "a1a53d30"
   },
   "source": [
    "### 2. Synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4776d52",
   "metadata": {
    "id": "a4776d52"
   },
   "outputs": [],
   "source": [
    "from ydata_synthetic.synthesizers.regular import RegularSynthesizer\n",
    "\n",
    "synth = RegularSynthesizer(modelname='fast')\n",
    "synth.fit(data=data, num_cols=num_cols, cat_cols=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c37f716",
   "metadata": {
    "id": "1c37f716"
   },
   "outputs": [],
   "source": [
    "synth_data = synth.sample(len(X_Hold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75787840",
   "metadata": {
    "id": "75787840"
   },
   "outputs": [],
   "source": [
    "synth_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d78bf",
   "metadata": {
    "id": "5c4d78bf"
   },
   "source": [
    "#### 2.2.1 Augmentation with Generative AI (CTGAN - Challenge)\n",
    "\n",
    "Because GANs training can take from minutes to hours depending on the size of the dataset and do they also require the use of GPU acceleration, we have used a faster synthesis method based on density methods.\n",
    "\n",
    "But what would be the results if we have used CTGAN to balance our data? Would it bring more variability and help our model generalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4efa67",
   "metadata": {
    "id": "ce4efa67"
   },
   "outputs": [],
   "source": [
    "#add here the code for the challenge.\n",
    "from ydata_synthetic.regular import RegularSynthesizer\n",
    "\n",
    "# Defining the training parameters\n",
    "batch_size = 500\n",
    "epochs = 500+1\n",
    "learning_rate = 2e-4\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.9\n",
    "\n",
    "ctgan_args = ModelParameters(batch_size=batch_size,\n",
    "                             lr=learning_rate,\n",
    "                             betas=(beta_1, beta_2))\n",
    "\n",
    "train_args = TrainParameters(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2eaae",
   "metadata": {
    "id": "65c2eaae"
   },
   "outputs": [],
   "source": [
    "#Training our Gan-based synthetic data generation method\n",
    "synth = RegularSynthesizer(modelname='ctgan',\n",
    "                           model_parameters=ctgan_args)\n",
    "\n",
    "synth.fit(data=data, train_arguments=train_args, num_cols=num_cols, cat_cols=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf43e38",
   "metadata": {
    "id": "cdf43e38"
   },
   "outputs": [],
   "source": [
    "#Decide what is the size of the synthetic sample that you expect to generate to balance the original data\n",
    "synth_data = synth.sample(num_sample=len(X_Hold))\n",
    "print(synth_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c689f",
   "metadata": {
    "id": "9f7c689f"
   },
   "source": [
    "## Step 3 - Synthetic data quality validation & iteration\n",
    "\n",
    "The whole process os **Synthetic Data** generation is not the same unless we evaluate the outputed data quality and iterate until the expected results are achieved - privacy, augmentation, de-biasing data,etc.\n",
    "\n",
    "To understand one of the dimensions of synthetic data quality, we will be again leveraging the ydata-profiling report.\n",
    "\n",
    "### Remember the holdout?\n",
    "\n",
    "Now that we have our synthetic data generated, it is time to evaluate its quality. For that reason we will be leveraging ydata-profiling report once again, but this time we will be comparing the original vs synthetic data report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd07ac7",
   "metadata": {
    "id": "4dd07ac7"
   },
   "outputs": [],
   "source": [
    "syntheticdata_report = ProfileReport(synth_data,\n",
    "                                    title='Census Synthetic Data',\n",
    "                                    minimal=True)\n",
    "\n",
    "compare_report = report.compare(syntheticdata_report)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "compare_report"
   ],
   "metadata": {
    "id": "avRpq9EsnyJa"
   },
   "id": "avRpq9EsnyJa",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8cf5f629",
   "metadata": {
    "id": "8cf5f629"
   },
   "source": [
    "### The importance of data pipelines\n",
    "\n",
    "Throughout our small tutorial we have checked how we can easily setup a flow for synthetic data generation. But in order for this flow to be reproducible and versionable we need something else - Pipelines!\n",
    "\n",
    "Data pipelines streamline the flow of data through various stages—from extraction and processing to modeling and storage—providing a structured environment for automating and monitoring each step. This structured approach is particularly crucial for synthetic data generation, which often involves complex simulations or algorithms to produce data that is both realistic and diverse.\n",
    "\n",
    "Effective data pipelines enable the generation of high-quality synthetic data by ensuring that the input data is correctly preprocessed and that the generation algorithms are executed under controlled and reproducible conditions. This results in synthetic datasets that closely mimic real-world data distributions without exposing sensitive information, thereby facilitating more accurate and ethical AI models.\n",
    "\n",
    "Moreover, scalable data pipelines are essential for handling large volumes of data, allowing organizations to generate and utilize synthetic data at a scale that matches their needs. By automating repetitive tasks, data pipelines also free up data scientists to focus on more strategic tasks such as improving data models and extracting valuable insights.\n",
    "\n",
    "Check how this can be done in a drag-and-drop manner in **YData Fabric** (https://ydata.ai/register).\n",
    "\n",
    "![ydata fabric pipelines](\"img/ydata_fabric_pipelines.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
